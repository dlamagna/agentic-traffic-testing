model_name: "meta-llama/Llama-3.1-8B-Instruct"
tensor_parallel_size: 1
max_model_len: 8192
dtype: "auto"
trust_remote_code: true

# You can pass additional vLLM engine arguments via CLI flags, e.g.:
#   python -m llm.serve_llm --tp-size 2 --max-model-len 16384




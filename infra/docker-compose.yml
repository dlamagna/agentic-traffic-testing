services:
  llm-backend:
    build:
      context: ..
      dockerfile: llm/Dockerfile
    container_name: llm-backend
    environment:
      - NODE_NAME=node3_llm
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_TOKEN=${HF_TOKEN}
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN:-${HF_TOKEN}}
      - LOG_LLM_REQUESTS=${LOG_LLM_REQUESTS:-0}
      - LLM_LOG_MAX_CHARS=${LLM_LOG_MAX_CHARS:-500}
      - LLM_MAX_CONCURRENCY=${LLM_MAX_CONCURRENCY:-1}
      - LLM_METRICS_ENABLED=${LLM_METRICS_ENABLED:-1}
      - LLM_METRICS_INCLUDE_TOKENS=${LLM_METRICS_INCLUDE_TOKENS:-1}
      - LLM_METRICS_PREFIX=${LLM_METRICS_PREFIX:-llm}
    command: ["python3", "-m", "llm.serve_llm", "--model", "meta-llama/Llama-3.1-8B-Instruct", "--host", "0.0.0.0", "--port", "8000", "--max-model-len", "${LLM_MAX_MODEL_LEN:-4000}"]
    ports:
      - "8000:8000"
    networks:
      - agent-net
    healthcheck:
      test:
        [
          "CMD",
          "python3",
          "-c",
          "import urllib.request; urllib.request.urlopen('http://localhost:8000/health', timeout=2).read()",
        ]
      interval: 5s
      timeout: 3s
      retries: 40
      start_period: 10s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: ["gpu"]

  agent-a:
    build:
      context: ..
      dockerfile: agents/Dockerfile
    container_name: agent-a
    environment:
      - LLM_SERVER_URL=http://llm-backend:8000/chat
      - NODE_NAME=node1_agentA
      - AGENT_A_PORT=8101
      - OTEL_SERVICE_NAME=agent-a
      - AGENT_B_URLS=http://agent-b:8102/subtask,http://agent-b-2:8103/subtask,http://agent-b-3:8104/subtask,http://agent-b-4:8105/subtask,http://agent-b-5:8106/subtask
      - LLM_TIMEOUT_SECONDS=${LLM_TIMEOUT_SECONDS:-120}
      - AGENT_B_TIMEOUT_SECONDS=${AGENT_B_TIMEOUT_SECONDS:-120}
      - LOG_LLM_REQUESTS=${LOG_LLM_REQUESTS:-0}
      - LLM_LOG_MAX_CHARS=${LLM_LOG_MAX_CHARS:-500}
    depends_on:
      llm-backend:
        condition: service_healthy
    ports:
      - "8101:8101"
    command: ["python", "-m", "agents.agent_a.server"]
    networks:
      - agent-net

  agent-b:
    build:
      context: ..
      dockerfile: agents/Dockerfile
    container_name: agent-b
    environment:
      - LLM_SERVER_URL=http://llm-backend:8000/chat
      - NODE_NAME=node2_agentB
      - AGENT_B_PORT=8102
      - OTEL_SERVICE_NAME=agent-b-1
      - LLM_TIMEOUT_SECONDS=${LLM_TIMEOUT_SECONDS:-120}
      - LOG_LLM_REQUESTS=${LOG_LLM_REQUESTS:-0}
      - LLM_LOG_MAX_CHARS=${LLM_LOG_MAX_CHARS:-500}
    depends_on:
      llm-backend:
        condition: service_healthy
    ports:
      - "8102:8102"
    command: ["python", "-m", "agents.agent_b.server"]
    networks:
      - agent-net

  agent-b-2:
    build:
      context: ..
      dockerfile: agents/Dockerfile
    container_name: agent-b-2
    environment:
      - LLM_SERVER_URL=http://llm-backend:8000/chat
      - NODE_NAME=node2_agentB_2
      - AGENT_B_PORT=8103
      - OTEL_SERVICE_NAME=agent-b-2
      - LLM_TIMEOUT_SECONDS=${LLM_TIMEOUT_SECONDS:-120}
      - LOG_LLM_REQUESTS=${LOG_LLM_REQUESTS:-0}
      - LLM_LOG_MAX_CHARS=${LLM_LOG_MAX_CHARS:-500}
    depends_on:
      llm-backend:
        condition: service_healthy
    ports:
      - "8103:8103"
    command: ["python", "-m", "agents.agent_b.server"]
    networks:
      - agent-net

  agent-b-3:
    build:
      context: ..
      dockerfile: agents/Dockerfile
    container_name: agent-b-3
    environment:
      - LLM_SERVER_URL=http://llm-backend:8000/chat
      - NODE_NAME=node2_agentB_3
      - AGENT_B_PORT=8104
      - OTEL_SERVICE_NAME=agent-b-3
      - LLM_TIMEOUT_SECONDS=${LLM_TIMEOUT_SECONDS:-120}
      - LOG_LLM_REQUESTS=${LOG_LLM_REQUESTS:-0}
      - LLM_LOG_MAX_CHARS=${LLM_LOG_MAX_CHARS:-500}
    depends_on:
      llm-backend:
        condition: service_healthy
    ports:
      - "8104:8104"
    command: ["python", "-m", "agents.agent_b.server"]
    networks:
      - agent-net

  agent-b-4:
    build:
      context: ..
      dockerfile: agents/Dockerfile
    container_name: agent-b-4
    environment:
      - LLM_SERVER_URL=http://llm-backend:8000/chat
      - NODE_NAME=node2_agentB_4
      - AGENT_B_PORT=8105
      - OTEL_SERVICE_NAME=agent-b-4
      - LLM_TIMEOUT_SECONDS=${LLM_TIMEOUT_SECONDS:-120}
      - LOG_LLM_REQUESTS=${LOG_LLM_REQUESTS:-0}
      - LLM_LOG_MAX_CHARS=${LLM_LOG_MAX_CHARS:-500}
    depends_on:
      llm-backend:
        condition: service_healthy
    ports:
      - "8105:8105"
    command: ["python", "-m", "agents.agent_b.server"]
    networks:
      - agent-net

  agent-b-5:
    build:
      context: ..
      dockerfile: agents/Dockerfile
    container_name: agent-b-5
    environment:
      - LLM_SERVER_URL=http://llm-backend:8000/chat
      - NODE_NAME=node2_agentB_5
      - AGENT_B_PORT=8106
      - OTEL_SERVICE_NAME=agent-b-5
      - LLM_TIMEOUT_SECONDS=${LLM_TIMEOUT_SECONDS:-120}
      - LOG_LLM_REQUESTS=${LOG_LLM_REQUESTS:-0}
      - LLM_LOG_MAX_CHARS=${LLM_LOG_MAX_CHARS:-500}
    depends_on:
      llm-backend:
        condition: service_healthy
    ports:
      - "8106:8106"
    command: ["python", "-m", "agents.agent_b.server"]
    networks:
      - agent-net

  mcp-tool-db:
    build:
      context: ..
      dockerfile: tools/mcp_tool_db/Dockerfile
    container_name: mcp-tool-db
    environment:
      - NODE_NAME=node2_toolDB
      - MCP_TOOL_DB_PORT=8201
    ports:
      - "8201:8201"
    networks:
      - agent-net

  chat-ui:
    build:
      context: ..
      dockerfile: ui/Dockerfile
    container_name: chat-ui
    ports:
      - "3000:3000"
    networks:
      - agent-net
    depends_on:
      - agent-a
      - agent-b
      - agent-b-2
      - agent-b-3
      - agent-b-4
      - agent-b-5

  jaeger:
    image: jaegertracing/all-in-one:1.57
    container_name: jaeger
    ports:
      - "16686:16686"   # Jaeger UI
      - "4317:4317"     # OTLP gRPC
      - "4318:4318"     # OTLP HTTP
    networks:
      - agent-net

networks:
  agent-net:
    driver: bridge




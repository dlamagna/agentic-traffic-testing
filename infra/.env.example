# =============================================================================
# Agentic Traffic Testbed - Environment Configuration
# =============================================================================
# Copy this file to .env and adjust values as needed:
#   cp .env.example .env
#
# The .env file is gitignored - keep secrets out of version control.
# =============================================================================

# -----------------------------------------------------------------------------
# DEPLOYMENT MODE
# -----------------------------------------------------------------------------
# Controls how services are deployed and networked.
#
# Options:
#   single      - All containers on one Docker bridge network (default)
#                 Simple setup, good for development and basic testing.
#
#   distributed - Separate Docker networks per logical node
#                 Agent A, Agent B, and LLM each get isolated networks,
#                 connected via an inter-agent network. Better for observing
#                 cross-service traffic patterns.
#
#   multi-vm    - Services deployed to separate VMs via SSH
#                 Requires NODE1_HOST, NODE2_HOST, NODE3_HOST to be set.
#                 Best for realistic network measurements.
#
DEPLOYMENT_MODE=distributed

# -----------------------------------------------------------------------------
# DISTRIBUTED MODE NETWORK CONFIGURATION
# -----------------------------------------------------------------------------
# Only used when DEPLOYMENT_MODE=distributed
# These define the subnet ranges for each logical node's network.
#
NETWORK_AGENT_A_SUBNET=172.20.0.0/24
NETWORK_AGENT_B_SUBNET=172.21.0.0/24
NETWORK_LLM_SUBNET=172.22.0.0/24
NETWORK_INTER_AGENT_SUBNET=172.23.0.0/24
NETWORK_TOOLS_SUBNET=172.24.0.0/24

# Static IPs for services in distributed mode
# Agent A network
AGENT_A_IP=172.20.0.10
AGENT_A_INTER_IP=172.23.0.10

# Agent B network (primary instance)
AGENT_B_IP=172.21.0.10
AGENT_B_INTER_IP=172.23.0.20

# Additional Agent B instances
AGENT_B_2_IP=172.21.0.11
AGENT_B_2_INTER_IP=172.23.0.21
AGENT_B_3_IP=172.21.0.12
AGENT_B_3_INTER_IP=172.23.0.22
AGENT_B_4_IP=172.21.0.13
AGENT_B_4_INTER_IP=172.23.0.23
AGENT_B_5_IP=172.21.0.14
AGENT_B_5_INTER_IP=172.23.0.24

# LLM backend
LLM_BACKEND_IP=172.22.0.10
LLM_BACKEND_INTER_IP=172.23.0.30

# MCP Tools (tools_network)
MCP_TOOL_DB_IP=172.24.0.10
MCP_TOOL_DB_INTER_IP=172.23.0.40

# Chat UI
CHAT_UI_IP=172.23.0.50

# Jaeger
JAEGER_IP=172.23.0.60

# -----------------------------------------------------------------------------
# MULTI-VM MODE CONFIGURATION
# -----------------------------------------------------------------------------
# Only used when DEPLOYMENT_MODE=multi-vm
# Set these to deploy services across separate VMs via SSH.
#
# NODE1_HOST=192.168.56.10    # Agent A + Jaeger + Chat UI
# NODE2_HOST=192.168.56.11    # Agent B instances + MCP tools
# NODE3_HOST=192.168.56.12    # LLM backend (GPU node)
# REMOTE_REPO_DIR=/home/user/projects/agentic-traffic-testing

# -----------------------------------------------------------------------------
# NETWORK EMULATION (tc netem)
# -----------------------------------------------------------------------------
# Add artificial latency/jitter/loss to make Docker networks more realistic.
# Only applies in distributed mode.
#
# Set to 1 to enable network emulation
ENABLE_NETWORK_EMULATION=1

# Delay in milliseconds (e.g., 10ms simulates local network)
NETWORK_DELAY_MS=10

# Jitter in milliseconds (variation in delay)
NETWORK_JITTER_MS=2

# Packet loss percentage (0-100)
NETWORK_LOSS_PERCENT=0

# -----------------------------------------------------------------------------
# LLM CONFIGURATION
# -----------------------------------------------------------------------------
# Model and runtime settings for the vLLM backend.
#
LLM_MODEL=meta-llama/Llama-3.1-8B-Instruct
LLM_MAX_MODEL_LEN=4096
LLM_DTYPE=float16
LLM_MAX_NUM_SEQS=5
LLM_MAX_NUM_BATCHED_TOKENS=8192
LLM_GPU_MEMORY_UTILIZATION=0.90
LLM_TIMEOUT_SECONDS=120

# -----------------------------------------------------------------------------
# HUGGING FACE AUTHENTICATION
# -----------------------------------------------------------------------------
# Required for gated models like Llama.
# Get your token at: https://huggingface.co/settings/tokens
#
HF_TOKEN=${HF_TOKEN}
HUGGINGFACE_HUB_TOKEN=${HF_TOKEN}

# -----------------------------------------------------------------------------
# AGENT CONFIGURATION
# -----------------------------------------------------------------------------
AGENT_B_TIMEOUT_SECONDS=120
MAX_AGENT_B_TURNS=3
MAX_PARALLEL_WORKERS=5

# -----------------------------------------------------------------------------
# LOGGING AND TELEMETRY
# -----------------------------------------------------------------------------
# Set to 1 to log LLM request/response content (verbose)
LOG_LLM_REQUESTS=0
LLM_LOG_MAX_CHARS=500

# LLM metrics
LLM_METRICS_ENABLED=1
LLM_METRICS_INCLUDE_TOKENS=1
LLM_METRICS_PREFIX=llm

# -----------------------------------------------------------------------------
# OBSERVABILITY
# -----------------------------------------------------------------------------
# OpenTelemetry / Jaeger endpoint (used by agents for tracing)
OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4317

# -----------------------------------------------------------------------------
# MONITORING (Prometheus + Grafana)
# -----------------------------------------------------------------------------
# Set to 1 to deploy Prometheus, Grafana, and cAdvisor for metrics visualization.
#
# Endpoints when enabled:
#   - Grafana:    http://localhost:3001 (admin/admin)
#   - Prometheus: http://localhost:9090
#   - cAdvisor:   http://localhost:8080
#
ENABLE_MONITORING=1
